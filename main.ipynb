{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b85a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (2.1.1)\n",
      "Requirement already satisfied: torch in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: shapely in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (2.0.7)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (8.3.86)\n",
      "Requirement already satisfied: redis in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (5.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\trydo\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python-headless numpy torch shapely ultralytics redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from shapely.geometry import Polygon, Point\n",
    "import redis\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dacb5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb75d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParkingSpaceMonitor:\n",
    "    def __init__(self, video_source, yolo_model_path, parking_id, parking_spaces_coords, crop_points=None, \n",
    "                 check_interval=10, consecutive_checks=3, confidence_threshold=0.5):\n",
    "        \"\"\"Инициализация системы мониторинга парковочных мест\"\"\"\n",
    "        self.video_source = video_source\n",
    "        self.yolo_model_path = yolo_model_path\n",
    "        self.parking_id = parking_id\n",
    "        self.parking_spaces_coords = parking_spaces_coords\n",
    "        self.crop_points = crop_points\n",
    "        self.check_interval = check_interval\n",
    "        self.consecutive_checks = consecutive_checks\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "        # Инициализация модели YOLO\n",
    "        self.model = YOLO(yolo_model_path)\n",
    "\n",
    "        # Только один класс - Vehicle с ID 0\n",
    "        self.vehicle_classes = [0]\n",
    "        self.class_names = {0: 'Vehicle'}\n",
    "\n",
    "        # Инициализация парковочных мест\n",
    "        self.parking_spaces = self._initialize_parking_spaces(parking_spaces_coords)\n",
    "\n",
    "        # Команда ffmpeg для обработки HLS\n",
    "        self.ffmpeg_command = [\n",
    "            'ffmpeg', '-i', self.video_source, '-loglevel', 'quiet', '-f', 'image2pipe',\n",
    "            '-pix_fmt', 'bgr24', '-vcodec', 'rawvideo', '-'\n",
    "        ]\n",
    "\n",
    "    def _initialize_parking_spaces(self, parking_spaces_coords):\n",
    "        \"\"\"Инициализация парковочных мест\"\"\"\n",
    "        parking_spaces = {}\n",
    "        for space_id, coords in parking_spaces_coords.items():\n",
    "            parking_spaces[space_id] = {\n",
    "                'polygon': Polygon(coords),\n",
    "                'status': 'free',\n",
    "                'history': []\n",
    "            }\n",
    "        return parking_spaces\n",
    "\n",
    "    def crop_frame(self, frame):\n",
    "        \"\"\"Обрезает изображение по заданным точкам\"\"\"\n",
    "        if not self.crop_points:\n",
    "            return frame\n",
    "        # Автоматически определяем размер по crop_points\n",
    "        width = int(np.linalg.norm(np.array(self.crop_points[1]) - np.array(self.crop_points[0])))\n",
    "        height = int(np.linalg.norm(np.array(self.crop_points[3]) - np.array(self.crop_points[0])))\n",
    "        src_points = np.array(self.crop_points, dtype=np.float32)\n",
    "        dst_points = np.array([[0, 0], [width, 0], [width, height], [0, height]], dtype=np.float32)\n",
    "        matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "        return cv2.warpPerspective(frame, matrix, (width, height))\n",
    "\n",
    "    def detect_objects(self, frame):\n",
    "        \"\"\"Обнаруживает объекты на кадре с помощью модели YOLO\"\"\"\n",
    "        results = self.model(frame)\n",
    "        result = results[0]\n",
    "        vehicles = []\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls.item())\n",
    "            confidence = float(box.conf.item())\n",
    "            if cls_id in self.vehicle_classes and confidence >= self.confidence_threshold:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                vehicles.append({\n",
    "                    'center': Point((x1 + x2) // 2, (y1 + y2) // 2),\n",
    "                    'bbox': (x1, y1, x2, y2),\n",
    "                    'class': self.class_names.get(cls_id, 'Vehicle'),\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        return vehicles\n",
    "\n",
    "    def check_parking_spaces(self, vehicles, occupation_threshold=0.4):\n",
    "        \"\"\"Проверяет занятость парковочных мест\"\"\"\n",
    "        current_status = {}\n",
    "        for space_id, space_info in self.parking_spaces.items():\n",
    "            space_polygon = space_info['polygon']\n",
    "            occupied_area = sum(\n",
    "                space_polygon.intersection(Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)])).area\n",
    "                for vehicle in vehicles\n",
    "                for x1, y1, x2, y2 in [vehicle['bbox']]\n",
    "                if space_polygon.intersects(Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)]))\n",
    "            )\n",
    "            occupation_percentage = occupied_area / space_polygon.area if space_polygon.area > 0 else 0\n",
    "            current_status[space_id] = 'occupied' if occupation_percentage >= occupation_threshold else 'free'\n",
    "        return current_status\n",
    "\n",
    "    def update_parking_status(self, current_status):\n",
    "        \"\"\"Обновляет статусы парковочных мест и возвращает список изменений\"\"\"\n",
    "        status_changes = []\n",
    "        for space_id, current in current_status.items():\n",
    "            history = self.parking_spaces[space_id]['history']\n",
    "            history.append(current)\n",
    "            if len(history) > self.consecutive_checks:\n",
    "                history.pop(0)\n",
    "            if len(history) == self.consecutive_checks and all(status == current for status in history):\n",
    "                if current != self.parking_spaces[space_id]['status']:\n",
    "                    status_changes.append((space_id, self.parking_spaces[space_id]['status'], current))\n",
    "                    self.parking_spaces[space_id]['status'] = current\n",
    "        return status_changes\n",
    "\n",
    "    def visualize(self, frame, parking_spaces):\n",
    "        \"\"\"Отображает на кадре статус парковочных мест\"\"\"\n",
    "        for space_id, space_info in parking_spaces.items():\n",
    "            coords = np.array(space_info['polygon'].exterior.coords, np.int32)\n",
    "            color = (0, 255, 0) if space_info['status'] == 'free' else (0, 0, 255)\n",
    "            cv2.polylines(frame, [coords], True, color, 2)\n",
    "            cv2.putText(frame, f\"ID: {space_id}\", coords[0], cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        return frame\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Обработка одного кадра\"\"\"\n",
    "        # Обрезаем кадр, если необходимо\n",
    "        frame = self.crop_frame(frame)\n",
    "\n",
    "        # Обнаружение объектов\n",
    "        vehicles = self.detect_objects(frame)\n",
    "\n",
    "        # Проверка занятости мест\n",
    "        current_status = self.check_parking_spaces(vehicles)\n",
    "\n",
    "        # Обновление статусов\n",
    "        status_changes = self.update_parking_status(current_status)\n",
    "\n",
    "        # Вывод изменений статусов\n",
    "        for space_id, old_status, new_status in status_changes:\n",
    "            print(f\"Парковочное место {space_id} изменило статус с '{old_status}' на '{new_status}'\")\n",
    "            key = f\"parking:{self.parking_id}:space:{space_id}\"\n",
    "            r.hset(key, mapping={\n",
    "                \"status\": new_status,\n",
    "                \"timestamp\": int(time.time())\n",
    "            })\n",
    "            r.publish(\n",
    "                f\"parking:{self.parking_id}:updates\",\n",
    "                json.dumps({\n",
    "                    \"space_id\": space_id,\n",
    "                    \"status\": new_status,\n",
    "                    \"timestamp\": int(time.time())\n",
    "                })\n",
    "            )\n",
    "\n",
    "        # Выделение машин на кадре\n",
    "        for vehicle in vehicles:\n",
    "            x1, y1, x2, y2 = vehicle['bbox']\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 165, 0), 2)  # Оранжевый цвет\n",
    "            label = f\"{vehicle['class']}: {vehicle['confidence']:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 165, 0), 2)\n",
    "\n",
    "        # Визуализация и возврат кадра\n",
    "        return self.visualize(frame, self.parking_spaces)\n",
    "\n",
    "    def run_hls_stream(self):\n",
    "        \"\"\"Обработка HLS-потока\"\"\"\n",
    "        pipe = subprocess.Popen(self.ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=10**8)\n",
    "        frame_size = (width, height)\n",
    "        frame_bytes = frame_size[0] * frame_size[1] * 3\n",
    "\n",
    "        # Предположим, что стрим ~25 FPS (можно сделать параметром)\n",
    "        fps = 25\n",
    "        frame_interval = int(fps * self.check_interval)\n",
    "        frame_count = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                raw_frame = pipe.stdout.read(frame_bytes)\n",
    "                if len(raw_frame) != frame_bytes:\n",
    "                    continue\n",
    "                frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((frame_size[1], frame_size[0], 3)).copy()\n",
    "\n",
    "                # Обрабатываем только каждый N-й кадр\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    cv2.imshow('Parking Monitor', processed_frame)\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "                frame_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка обработки кадра: {str(e)}\")\n",
    "                continue\n",
    "        pipe.terminate()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def run_video_file(self):\n",
    "        \"\"\"Обработка видеофайла\"\"\"\n",
    "        cap = cv2.VideoCapture(self.video_source)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_interval = int(fps * self.check_interval)\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Обрабатываем только каждый N-й кадр\n",
    "            if frame_count % frame_interval == 0:\n",
    "                processed_frame = self.process_frame(frame)\n",
    "                cv2.imshow('Parking Monitor', processed_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            frame_count += 1\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Запуск обработки в зависимости от типа источника\"\"\"\n",
    "        if self.video_source.startswith('http'):\n",
    "            self.run_hls_stream()\n",
    "        else:\n",
    "            self.run_video_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19a9c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 (no detections), 319.3ms\n",
      "Speed: 5.0ms preprocess, 319.3ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 221.2ms\n",
      "Speed: 10.2ms preprocess, 221.2ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 143.3ms\n",
      "Speed: 4.1ms preprocess, 143.3ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 321.4ms\n",
      "Speed: 12.3ms preprocess, 321.4ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 153.2ms\n",
      "Speed: 3.2ms preprocess, 153.2ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 151.9ms\n",
      "Speed: 2.8ms preprocess, 151.9ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 134.9ms\n",
      "Speed: 4.7ms preprocess, 134.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 136.8ms\n",
      "Speed: 3.6ms preprocess, 136.8ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 146.2ms\n",
      "Speed: 3.0ms preprocess, 146.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 144.0ms\n",
      "Speed: 3.1ms preprocess, 144.0ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 134.7ms\n",
      "Speed: 2.5ms preprocess, 134.7ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 130.4ms\n",
      "Speed: 2.4ms preprocess, 130.4ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 131.1ms\n",
      "Speed: 2.4ms preprocess, 131.1ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 135.5ms\n",
      "Speed: 2.5ms preprocess, 135.5ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 128.3ms\n",
      "Speed: 2.7ms preprocess, 128.3ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 168.0ms\n",
      "Speed: 5.3ms preprocess, 168.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 153.8ms\n",
      "Speed: 4.3ms preprocess, 153.8ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 160.8ms\n",
      "Speed: 3.4ms preprocess, 160.8ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 215.6ms\n",
      "Speed: 3.7ms preprocess, 215.6ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 115.0ms\n",
      "Speed: 244.5ms preprocess, 115.0ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 148.4ms\n",
      "Speed: 2.2ms preprocess, 148.4ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 174.5ms\n",
      "Speed: 8.5ms preprocess, 174.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 115.5ms\n",
      "Speed: 2.7ms preprocess, 115.5ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 110.5ms\n",
      "Speed: 2.4ms preprocess, 110.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 141.5ms\n",
      "Speed: 2.4ms preprocess, 141.5ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 122.8ms\n",
      "Speed: 3.4ms preprocess, 122.8ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 121.9ms\n",
      "Speed: 2.2ms preprocess, 121.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 123.9ms\n",
      "Speed: 2.3ms preprocess, 123.9ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 133.2ms\n",
      "Speed: 2.5ms preprocess, 133.2ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 121.5ms\n",
      "Speed: 2.8ms preprocess, 121.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 130.4ms\n",
      "Speed: 2.3ms preprocess, 130.4ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 123.9ms\n",
      "Speed: 2.2ms preprocess, 123.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 107.5ms\n",
      "Speed: 4.5ms preprocess, 107.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 153.2ms\n",
      "Speed: 2.3ms preprocess, 153.2ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 122.4ms\n",
      "Speed: 2.1ms preprocess, 122.4ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 256.0ms\n",
      "Speed: 2.9ms preprocess, 256.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 135.7ms\n",
      "Speed: 2.7ms preprocess, 135.7ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 122.1ms\n",
      "Speed: 2.2ms preprocess, 122.1ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 119.1ms\n",
      "Speed: 2.3ms preprocess, 119.1ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 193.6ms\n",
      "Speed: 4.5ms preprocess, 193.6ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 238.0ms\n",
      "Speed: 2.5ms preprocess, 238.0ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 156.0ms\n",
      "Speed: 4.4ms preprocess, 156.0ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 415.1ms\n",
      "Speed: 48.1ms preprocess, 415.1ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 243.0ms\n",
      "Speed: 3.5ms preprocess, 243.0ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 179.2ms\n",
      "Speed: 3.0ms preprocess, 179.2ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 124.7ms\n",
      "Speed: 2.3ms preprocess, 124.7ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 116.9ms\n",
      "Speed: 4.6ms preprocess, 116.9ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 135.2ms\n",
      "Speed: 5.6ms preprocess, 135.2ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 139.7ms\n",
      "Speed: 3.2ms preprocess, 139.7ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 114.0ms\n",
      "Speed: 3.2ms preprocess, 114.0ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 181.5ms\n",
      "Speed: 2.1ms preprocess, 181.5ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 164.4ms\n",
      "Speed: 3.1ms preprocess, 164.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 115.0ms\n",
      "Speed: 2.4ms preprocess, 115.0ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 259.7ms\n",
      "Speed: 5.1ms preprocess, 259.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Инициализируем и запускаем мониторинг\u001b[39;00m\n\u001b[32m     15\u001b[39m monitor = ParkingSpaceMonitor(\n\u001b[32m     16\u001b[39m     video_source=\u001b[33m'\u001b[39m\u001b[33mhttps://cameras.inetcom.ru/hls/camera12_4.m3u8\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Путь к видеофайлу или HLS-поток https://cameras.inetcom.ru/hls/camera12_4.m3u8\u001b[39;00m\n\u001b[32m     17\u001b[39m     yolo_model_path=\u001b[33m'\u001b[39m\u001b[33mcar-75e-11n.pt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     confidence_threshold=\u001b[32m0.5\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmonitor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mParkingSpaceMonitor.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Запуск обработки в зависимости от типа источника\"\"\"\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.video_source.startswith(\u001b[33m'\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_hls_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_video_file()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mParkingSpaceMonitor.run_hls_stream\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Обрабатываем только каждый N-й кадр\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_count % frame_interval == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     processed_frame = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     cv2.imshow(\u001b[33m'\u001b[39m\u001b[33mParking Monitor\u001b[39m\u001b[33m'\u001b[39m, processed_frame)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cv2.waitKey(\u001b[32m1\u001b[39m) & \u001b[32m0xFF\u001b[39m == \u001b[38;5;28mord\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mParkingSpaceMonitor.process_frame\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m    109\u001b[39m frame = \u001b[38;5;28mself\u001b[39m.crop_frame(frame)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Обнаружение объектов\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m vehicles = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetect_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Проверка занятости мест\u001b[39;00m\n\u001b[32m    115\u001b[39m current_status = \u001b[38;5;28mself\u001b[39m.check_parking_spaces(vehicles)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mParkingSpaceMonitor.detect_objects\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_objects\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame):\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Обнаруживает объекты на кадре с помощью модели YOLO\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     result = results[\u001b[32m0\u001b[39m]\n\u001b[32m     54\u001b[39m     vehicles = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\model.py:182\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    155\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    156\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    157\u001b[39m     **kwargs: Any,\n\u001b[32m    158\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    159\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\model.py:560\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    263\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\predictor.py:145\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    140\u001b[39m visualize = (\n\u001b[32m    141\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    144\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\nn\\autobackend.py:557\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    154\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\nn\\modules\\conv.py:55\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     54\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Пример использования:\n",
    "    # Определяем координаты парковочных мест\n",
    "    # Формат: {id_места: [(x1, y1), (x2, y2), (x3, y3), (x4, y4)], ...}\n",
    "parking_spaces = {\n",
    "    1: [(348, 73), (402, 77), (423, 119), (359, 111)],\n",
    "    2: [(293, 60), (346, 70), (355, 108), (285, 100)],\n",
    "    3: [(232, 49), (289, 59), (283, 107), (207, 96)],\n",
    "    4: [(176, 48), (235, 52), (206, 95), (140, 89)],\n",
    "    5: [(127, 43), (177, 48), (139, 88), (86, 84)],\n",
    "}\n",
    "\n",
    "crop_points = [(1340, 560), (1860, 560), (1860, 960), (1340, 960)]\n",
    "    \n",
    "    # Инициализируем и запускаем мониторинг\n",
    "monitor = ParkingSpaceMonitor(\n",
    "    video_source='https://cameras.inetcom.ru/hls/camera12_4.m3u8',  # Путь к видеофайлу или HLS-поток https://cameras.inetcom.ru/hls/camera12_4.m3u8\n",
    "    yolo_model_path='car-75e-11n.pt',\n",
    "    parking_id='0',\n",
    "    parking_spaces_coords=parking_spaces,\n",
    "    crop_points=crop_points,  \n",
    "    check_interval=2,\n",
    "    consecutive_checks=2,\n",
    "    confidence_threshold=0.5\n",
    ")\n",
    "monitor.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
